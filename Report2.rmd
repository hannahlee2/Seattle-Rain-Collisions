---
title: "Report 2"
author: "Hannah Lee, Khyati Jariwala, Michelle Xu"
date: "2022-12-07"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print=100))
```

## Introduction

The city of Seattle probably makes you think about a few things: Starbucks, rainy weather, and popular tourist attractions, like the Space Needle. This got us thinking, what more can we uncover about Seattle using seemingly unrelated data like weather, collisions, and population metrics to examine potential relationships between these things.

The datasets we will be using are `Observed Monthly Rain Gauge Accumulations - Oct 2002 to May 2017`, which records monthly accumulations of rain gauges located throughout Seattle city limits, the `SDOT Collisions - All Years`, which records the number of collisions in Seattle (provided by SPD and recorded by Traffic Records), and the `City Annual Stats`, which includes the total population (and change in population), housing, and jobs for the City of Seattle for each calendar year. These datasets were retrieved from the city government’s open access databases *data-seattlecitygis.opendata.arcgis.com* and *data.seattle.gov*.

These datasets are of interest to us, because we want to examine whether seemingly different variables, such as the weather, collisions, and population metrics can have potential relationships with each other. For the purpose of simplicity, we have selected these metrics and data for the city of Seattle rather than a whole country. 

Before analyzing the data, we have first reshaped and tidied each of the datasets, and then joined them together by the variable for year, which is the common key between all of our datasets. Two datasets contain `month` and `day` data, so they were joined by the `month`, `day`, and `year` keys prior to being joined with the City Annual Stats dataset, which only has the `year` key. All datasets are predominantly numeric (rainfall, population, etc.), however there are a few categorical variables interspersed (year, collision severity, etc.).

A potential trend we expect to see once we combine the datasets is that the number of collisions should increase as the average rainfall increases. This is because rain causes roads to be slippery, which could result in collisions. Additionally, the more rainfall there is, the severity of the collision should also increase for the same reason. For population and collisions, we expect that with an increasing population the number of collisions would increase, because there would be more people on the road. Within the City Annual Stats dataset, we expect to see that the number of jobs will increase as the total population increases due to the fact that people are attracted to live in cities with more job opportunities (since people need to be able to make a living).

Using our datasets, we are interested in answering the following research questions,

**1. Is the severity of collisions in Seattle correlated with the average rainfall in that area?**

**2. What are the best predictors for the severity of a collision using a logistic regression model?**

---

## Data Cleaning

```{r,include=FALSE}
## Tidying
```


```{r, output=F}
library(tidyverse)

# Read in datasets
collisions <- read.csv("Collisions.csv") %>% as.data.frame()
citystats <- read.csv("City_Annual_Stats.csv") %>% as.data.frame()
rainfall <- read.csv("Observed_Monthly_Rain_Gauge_Accumulations_-_Oct_2002_to_May_2017.csv") %>% as.data.frame()
```

```{r,include=FALSE}
# Inidivudual dataset tidying ommitted due to page limit.
```


```{r}
# Tidying "Collisions" dataset
collisions %>%
  group_by(INCDTTM, COLLISIONTYPE, SEVERITYDESC, WEATHER, VEHCOUNT) %>% # there are columns we wanted to keep 
  summarise(n = n()) %>% # we reduced this exceedingly large dataset by transforming it around a different ID variable -- Date
  separate(INCDTTM, into = c("Month", "Day", "Year")) %>%
  mutate_at(c("Year", "Month", "Day"), as.integer) %>% 
  filter(COLLISIONTYPE != "") %>% 
  arrange(n) %>% 
  ungroup() -> collisions_cleaned
```

```{r}
# Tidying city stats by keeping only useful columns of dataset
citystats %>%
  select(-c(City, Year_Display, ObjectID, Change_Population, Change_Housing_Units, Change_Jobs)) -> city_cleaned
```

```{r}
# Tidying rainfall dataset
rainfall %>%
  pivot_longer(cols = c("RG01":"RG20_25"),
               names_to = "Gauge_Location",
               names_transform = as.factor,
               values_to = "Accumulated_Rainfall",
               values_transform = as.numeric) %>%
  separate(Date, into = c("Month", "Day", "Year")) %>%
  mutate_at(c("Year", "Month", "Day"), as.integer) %>% 
  group_by(Year, Month) %>% 
  summarise(Rain_Accum = mean(Accumulated_Rainfall)) %>% 
  ungroup()-> rain_cleaned # we think its in in
```


```{r,include=FALSE}
# To begin, we modified two of our datasets so that they are tidy. For the `Observed Monthly Rain Gauge Accumulations - Oct 2002 to May 2017` dataset, we created a new variable for *Gauge_Location* and for *Accumulated_Rainfall*, which contains the corresponding amount of rainfall accumulated for each location. Both the `Observed Monthly Rain Gauge Accumulations - Oct 2002 to May 2017` and `SDOT Collisions - All Years` datasets provided dates in the format of MM/DD/YY. Our goal is to join all of our datasets by year, so we separated the given dates into variables for month, day, and year.
```

```{r,include=FALSE}
## Joining/Merging
```


```{r,include=FALSE}
# The total number of observations and unique IDs in the `citystats`, `rain_cleaned` and `collisions_cleaned` datasets are 2,519, 1,753, 1,935,458, respectively. The only ID that all of the datasets have in common is *Year*. While both `rain_cleaned` and `collisions_cleaned` have a *Month* ID, only `collisions_cleaned` has a *Day* ID. The unique IDs that appear in only the `citystats` dataset include *City*, *Const_Res*, *FIRE*, *Manufacturing*, *Retail*, *Services*, *WTU*, *Government*, *Education*, *Total_Jobs*, *Housing_Units*, *Total_Population*, *Households*, *Year_Display*, *Change_Population*, *Change_Jobs*, and *ObjectID*. The unique ID that appears only in the `rain_cleaned` dataset is *Rain_Accum*. Lastly, the unique IDs that appear only in the `collisions_cleaned` dataset are *COLLISIONTYPE*, *SEVERITYDESC*, *WEATHER*, *VEHCOUNT*, and *n*. There were no IDs that have been left out or any rows that were dropped/added while joining the datasets. Note that we removed a few columns that did not seem to provide meaningful information (particularly from the `citystats` dataset).
```


```{r,include=FALSE}
# find total number of observations for each dataset and unique IDs
dim(city_cleaned)
dim(rain_cleaned)
dim(collisions_cleaned)
```


```{r}
# joining cleaned data sets, where each row now represents a collision incident
collisions_cleaned %>% 
  left_join(rain_cleaned, by = c("Year", "Month")) %>% 
  left_join(city_cleaned, by = "Year") -> city_rain_collisions
```


```{r,include=FALSE}
# finding dimensions of joint dataset
dim(city_rain_collisions)
```

```{r,include=FALSE}
# viewing joint dataset
city_rain_collisions
```


```{r,include=FALSE}
## Wrangling

# viewing "working" dataset
city_rain_collisions
```

```{r,include=FALSE}
# create table of summary statistics for numerical variables, such as rainfall, vehicles in collisions, severity of collisions, jobs/population ratio per year
city_rain_collisions %>%
  select(Year, Rain_Accum, Total_Population, Total_Jobs, SEVERITYDESC, WEATHER, VEHCOUNT) %>%
  filter(Year %in% c(2002:2017)) %>%
  mutate(Ratio_jobs_per_pop = Total_Jobs / Total_Population) %>%
  ungroup() %>% 
  na.omit() %>% 
  summarise_if(is.numeric, list(mean = mean, sd = sd, min = min, max = max))

# For the variable *VEHCOUNT*, the vehicles affected in a collision, has a mean of 1.961 vehicles and a standard deviation of 0.589 vehicles. Using the mutate dplyr function, we were able to find the ratio of total number of jobs to the total population is 0.803 on average with a standard deviation of 0.02. For the variable *Rain_Accum*, we found that the monthly rain gauge accumulation is 5.3 inches on average with a standard deviation of 2.89 inches.
```

```{r,include=FALSE}
# create frequency tables for collision type and weather (summary statistics for categorical variables)
city_rain_collisions %>%
  select(COLLISIONTYPE) %>%
  group_by(COLLISIONTYPE) %>%
  summarise(Frequency = n()) %>% 
  arrange(desc(Frequency))

city_rain_collisions %>%
  select(WEATHER) %>%
  group_by(WEATHER) %>%
  summarise(Frequency = n()) %>% 
  arrange(desc(Frequency))

# Furthermore, we created a frequency table for the two categorical variables we were interested in, *COLLISIONTYPE* and *WEATHER*. These tables represent the number of collisions observed for each collision type and weather condition. In our dataset, we can see that the collisions most frequently occurred with parked cars and least frequently occurred when it was head-on. Additionally, in our dataset, the most collisions occurred when the weather was clear, and the least collisions occurred when there was blowing snow.
```


---


## Exploratory Data Analysis
All possible pairs of the variables `Year`, `Total_Population`, `Total_Jobs`, `Housing_Units`, and `Households` exhibit strong positive correlations. 
Additionally, `FIRE` and `Manufacturing` have strong negative correlation. Unfortunately, we cannot contextualize this finding accurately, as those data points were provided by the Washington State Employment Security Department without clear parameter definitions. 
The variables `Rain_Accum` and `VEHCOUNT` that are the least correlated with all other numerical variables. Based on our visualizations, the categorical variables do not have any apparent trends. 


```{r}
# prepare data for visualization
city_rain_collisions %>% mutate(Ratio_jobs_per_pop = Total_Jobs / Total_Population) -> df
```

```{r}
# Weather vs. Vehicle Count boxplot (2 variable)
df %>%
  filter(SEVERITYDESC != "Unknown") %>%
  ggplot(aes(x=SEVERITYDESC, y = VEHCOUNT, fill = SEVERITYDESC)) +
  geom_boxplot(alpha = 0.2) +
  scale_y_continuous(breaks = seq(0, 20, 2)) +
  labs(x = "Severity Description of Collision", 
       y = "Vehicle Count in Collision", 
       title = "Severity Description of Collision vs Vehicle Count in Seattle") +
  theme_minimal() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5))
```

```{r}
library(scales)
df %>%
  filter(WEATHER != "Blowing Sand/Dirt" & WEATHER != "Blowing Snow" & WEATHER != "Unknown" & WEATHER != "") %>%
  ggplot(aes(x=VEHCOUNT)) +
  geom_bar(aes(fill = WEATHER)) +
  ylim(0,150000)+
  labs(
    x = "Vehicle Count in Collision",
    y = "Count (Collisions in thousands)",
    title = "Weather Condition of Collisions by Vehicle Count"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Collisions vs. Day by Severity (2 variables)
df %>%
  ggplot(aes(x=Day)) +
  geom_bar(aes(fill = SEVERITYDESC)) +
  scale_x_continuous(breaks = seq(1, 31, 2)) +
  scale_y_continuous(breaks = seq(0, 7000, 1000)) +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    x = "Day of the Month",
    y = "Count (Collisions)",
    title = "Severity of Collision Incidents by Day"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Severity vs Population (2 variables)
df %>%
  filter(SEVERITYDESC != "Unknown") %>% 
  ggplot(aes(x=SEVERITYDESC, y = Total_Population, fill = SEVERITYDESC)) +
  geom_boxplot(alpha = 0.2) +
  scale_y_continuous(breaks = seq(500000,800000,25000)) +
  labs(x = "Severity Description of Collision", 
       y = "Total Population", 
       title = "Severity Description of Collision vs Total Population in Seattle") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 25, 
                                   hjust = 1), 
        legend.position = "none",
        plot.title = element_text(hjust = 0.5))
```

```{r}
# Jobs vs. Population by Year (3 variables)
df %>%
  ggplot(aes(x=Total_Population, y = Total_Jobs)) +
  geom_point(aes(color = Year)) +
  scale_color_gradient2(low = "blue", mid = "red", midpoint = 2006) +
  labs(x = "Total Population", y = "Total Jobs", title = "Total Population vs. Total Jobs in Seattle") +
  ylim(450000,650000) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Population/Housing vs. Year (3 variables)
library(scales)
df %>% 
  select(Year, Total_Population, Housing_Units) %>% 
  distinct() %>% 
  pivot_longer(
    cols = c("Total_Population", "Housing_Units"),
    names_to = "Category",
    values_to = "Stat"
  )  %>% 
  ggplot(aes(x = Year, y = Stat)) +
  geom_line(aes(color = Category),
            stat = "identity",
            alpha = 0.5,
            linetype = "longdash") +
  geom_text(aes(label=Stat,
                color = Category),
            stat='identity',
            angle = -25,
            vjust = -0.5,
            size = 2.5) +
  scale_x_continuous(breaks = seq(2004, 2022, 2)) +
  scale_y_continuous(labels = label_number(scale = 1e-3)) +
  labs(
    x = "Year",
    y = "Count (in thousands)",
    title = "Total Population and Housing Unit Trends Over Year") +
  theme_minimal() +
  theme( plot.title = element_text(hjust = 0.5) )
```

```{r}
# Correlation Matrix
# keep relevant numeric variables
df_numeric <- df %>%
  select(where(is.numeric)) %>%
  select(-c(n, Ratio_jobs_per_pop, Month, Day, Const_Res, WTU))
cor(df_numeric, use = "pairwise.complete.obs") %>%
  # Save as a data frame
  as.data.frame %>%
  # Convert row names to an explicit variable
  rownames_to_column %>%
  # Pivot so that all correlations appear in the same column
  pivot_longer(-1, 
               names_to = "other_var", 
               values_to = "correlation") %>%
  # Define ggplot (reorder values on y-axis)
  ggplot(aes(x = rowname, 
             y = ordered(other_var, levels = rev(sort(unique(other_var)))),
             fill = correlation)) +
  # Heat map with geom_tile
  geom_tile() +
  # Change the scale to make the middle appear neutral
  scale_fill_gradient2(low = "red", mid = "white", high = "blue") +
  # Overlay values
  geom_text(aes(label = round(correlation,2)), color = "black", size = 4) +
  # Angle the x-axis label to 45 degrees
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5)) +
  # Give title and labels
  labs(title = "Correlation matrix for Seattle Statistics", 
       x = "Variable 1", y = "Variable 2")
```


---


## Clustering 

Prior to beginning the clustering, we expected the clustering to occur on the basis of the severity of a collision. However, we were surprised to see that the graph depicting a silhouette width peaked at 0.4 with an optimal number of clusters being 2 when there are 4 types of severity. We then ran a PAM algorithm to get the clustering assignments and created the visualization shown in the 2D plot. The plot surprisingly depicted clustering based on years, with a divide (or central observation) around 2012. The delineation suggests that there was a major shift after 8 years that occurred in the city of Seattle, however, it is difficult to interpret why it occurs, as the centers of each cluster simply represent a specific count of collision with surrounding observations sharing similar timeframes. We suspect the reason for this shift and clustering based on years could be attributed to city development reasons, change in transportation patterns, etc, which our data does not capture.


```{r}
library(cluster)
library(factoextra)

df_reduced <- df %>%
  # Drop some variables
  select(-c(n, Month, Day, FIRE, Const_Res, Manufacturing, Retail, Services, WTU, Government, Education, Ratio_jobs_per_pop)) %>%
  # Consider categorical variables as factors
  mutate_if(is.character, as.factor) %>%
  # Ignore missing values
  drop_na

# reduce the number of rows just for the purposes of clustering
df_reduced1 <- head(df_reduced, 900)

# Calculate Gower distances between observations
df_reduced1 %>%
  daisy(metric = "gower") %>%
  # Save as a matrix
  as.matrix -> df_gower

# Use the silhouette on the matrix of distances
fviz_nbclust(df_gower, pam, method = "silhouette") +
  theme(plot.title = element_text(hjust = 0.5))

# Apply PAM on the dissimilarity object (specify diss = TRUE)
pam_results <- pam(df_gower, k = 2, diss = TRUE)

# Save cluster assignment as a column in the dataset
df_pam <- df_reduced1 %>%
  mutate(cluster = as.factor(pam_results$clustering))

# Summary statistics of numeric variables
df_pam %>%
  group_by(cluster) %>%
  summarize_if(is.numeric, mean, na.rm = T)

# ggplot visualization
ggplot(df_pam, aes(x=Year, y=SEVERITYDESC, color=cluster)) + 
  geom_point() + 
  theme_minimal() +
  labs(title = "Clustering Based on Years") +
  theme(plot.title = element_text(hjust = 0.5))

# table to visualize clustering
df_pam %>%
  group_by(Year) %>%
  select(cluster) %>%
  table() %>% 
  as.data.frame() %>% 
  pivot_wider(
    names_from = Year,
    values_from = Freq
  ) %>% 
  knitr::kable(format="markdown")
```


---


## Dimensionality reduction

The PCA was performed on all numeric variables. The first PC explains the most variation at about 68.8%. The second PC explains 14.3% of the variation. The first 3 PCs (1-3) collectively explain 96.6% of the variation. The contribution graphs help to interpret the PCs. PC1’s top contributing variables are households, total population, housing units, the year, and total jobs. PC2’s top contributing variable is VEHCOUNT. These observations can also be affirmed by the correlation circle, which shows vectors representing households, total population, housing units, the year, and total jobs pointing in the positive direction of the Dim 2 axis with almost perfect parallel, and the VEHCOUNT vector points in the negative direction of the Dim 1 axis with almost perfect parallel. In addition, the correlation circle shows  orthogonality of the VEHCOUNT vector to almost all the other vectors, indicating its weak correlation with other variables as affirmed by the correlation matrix. Rain_Accum also sees a weak correlation with both PC’s, which we can see shown by its lack of parallel alignment to either axis of the correlation circle, and also affirmed by the correlation matrix previously provided. Overall, considering both PCs as a whole using the correlation circle graph above, the relationship between variables depicted echos findings from the contribution graphs as well as the correlation matrix.


```{r}
library(factoextra)
library(gridExtra)
library(grid)
library(lattice)
# Apply PCA to numeric variables only
pca <- df_reduced %>%
  select_if(is.numeric) %>%
  scale %>% # remember to scale
  prcomp

# Percentage of variance explained for each PC in a scree plot
dr1 <- fviz_eig(pca, addlabels = TRUE, ylim = c(0, 70)) +
  theme(plot.title = element_text(hjust = 0.5))

# Correlation circle
dr2 <- fviz_pca_var(pca, col.var = "black", 
                    labelsize = 2,
                    repel = TRUE) + # Avoid text overlapping of the variable names
  theme(plot.title = element_text(hjust = 0.5))

# Top contributions of the variables to the PC as a percentage
dr3 <- fviz_contrib(pca, choice = "var", axes = 1, top = 5) +
  theme(plot.title = element_text(hjust = 0.5)) # on PC1
dr4 <- fviz_contrib(pca, choice = "var", axes = 2, top = 5) +
  theme(plot.title = element_text(hjust = 0.5)) # on PC2

grid.arrange(dr1, dr2, dr3, dr4, ncol=2)
```


## Classification and Cross-Validation

Since we did not have a binary variable, we converted our severity description of collisions variable into a binary variable, where a 1 was considered to be a severe collision and a 0 was considered to be a non-severe collision. We picked a logistic regression model instead of a kNN model because binary variables seem to have better predictability with logistic regressions. According to the AUC of the ROC curve, the logistic regression model predicts the severity description of new collision observations very well, as expected. 

```{r}

# make severity description a binary variable
df = df_reduced %>%
  mutate(SEVERITYDESC = ifelse(SEVERITYDESC == "Serious Injury Collision" | SEVERITYDESC == "Fatality Collision", 1, 0))

# train a logistic regression model using whole dataset
# Fit the model
fit_log <- glm(SEVERITYDESC~., data = df, family = "binomial")

# Take a look at the model summary
library(jtools)
summ(fit_log)


# Calculate a predicted value
df_pred <- df %>% 
  mutate(predictions = predict(fit_log, type = "response"),
         predicted = ifelse(predictions > 0.5, 1, 0)) %>%
  select(SEVERITYDESC, COLLISIONTYPE, WEATHER, predictions, predicted)

library(plotROC)
# ROC curve
ROC <- ggplot(df_pred) + 
  geom_roc(aes(d = SEVERITYDESC, m = predictions), n.cuts = 10) +
  labs(title = "ROC Curve for Logistic Regression Model on Seattle Dataset")
ROC +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Calculate the area under the curve
calc_auc(ROC)$AUC
```

We then performed k-fold cross-validation to get an average performance and check if our model was overfitting. Since the average performance of the k-fold cross-validation performance was very similar to the AUC of the ROC curve for our logistic regression model, so there were no clear signs of overfitting.

```{r}
# perform cross-validation on model using k-folds
# Choose number of folds: let's choose 5 since n is so small
k = 5 

# Randomly order rows in the dataset
data <- df[sample(nrow(df)), ] 

# Create k folds from the dataset
folds <- cut(seq(1:nrow(data)), breaks = k, labels = FALSE) 

# Use a for loop to get diagnostics for each test set
perf_k <- NULL

for(i in 1:k){
  # Create train and test sets
  train <- data[folds != i, ] # all observations except in fold i
  test <- data[folds == i, ]  # observations in fold i
  
  # Train model on train set (all but fold i)
  fit <- glm(SEVERITYDESC~., data = train, family = "binomial")
  
  # Test model on test set (fold i)
  df <- data.frame(
    predictions = predict(fit, newdata = test, type = "response"),
    SEVERITYDESC = test$SEVERITYDESC)
  
  # Consider the ROC curve for the test dataset
  ROC <- ggplot(df) + geom_roc(aes(d = SEVERITYDESC, m = predictions))
 
  # Get diagnostics for fold i (AUC)
  perf_k[i] <- calc_auc(ROC)$AUC
}

# Average performance 
mean(perf_k)

```


---


## References
1) Observed Monthly Rain Gauge Accumulations - Oct 2002 to May 2017: This data source includes monthly accumulations of rain gauges located throughout Seattle city limits. https://data.seattle.gov/City-Business/Observed-Monthly-Rain-Gauge-Accumulations-Oct-2002/rdtp-hzy3 
2) SDOT Collisions - All Years: This data source includes records of collisions in Seattle (provided by SPD and recorded by Traffic Records). 
 https://data-seattlecitygis.opendata.arcgis.com/datasets/sdot-collisions-all-years/explore?location=47.614507%2C-122.333041%2C12.33 
3) City Annual Stats: This dataset includes information of the total population (and change in population), housing, and jobs for the City of Seattle by year. 
https://data-seattlecitygis.opendata.arcgis.com/datasets/SeattleCityGIS::city-annual-stats-2/explore 


---


## Acknowledgements
Khyati Jariwala performed the classification/cross-validation methods and organized the presentation for our findings. Michelle Xu and Hannah Lee created visualizations, performed clustering, and interpreted the findings. 